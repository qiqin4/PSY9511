{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preparation\n",
    "Before any analyses we will prepare the dataset for the subsequent modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Auto dataset into R or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Auto=pd.read_csv(\"Auto.csv\")\n",
    "Auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Drop all variables except the (potential) predictors ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year'] and the target variable 'mpg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the predictors and the outcome varibales \n",
    "predictors = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year']\n",
    "target = 'mpg'\n",
    "auto = Auto[predictors + [target]]\n",
    "auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the dataset into a training set (80%) and a validation set (20%). It is probably a good idea to set a random seed and shuffle the dataset prior to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset to training and validation sets(80%,20%)\n",
    "train_data, val_data = train_test_split(auto, test_size=0.2, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Replace missing values (coded as '?') in the both datasets with the mean of the given variable in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cylinders: 5.43 (1.71)\n",
      "displacement: 191.34 (104.02)\n",
      "horsepower: 104.22 (38.47)\n",
      "weight: 2949.35 (847.54)\n",
      "acceleration: 15.45 (2.72)\n",
      "year: 76.00 (3.71)\n",
      "cylinders after: 0.00 (1.00)\n",
      "displacement after: -0.00 (1.00)\n",
      "horsepower after: 0.00 (1.00)\n",
      "weight after: 0.00 (1.00)\n",
      "acceleration after: 0.00 (1.00)\n",
      "year after: 0.00 (1.00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check ? values\n",
    "# print((train_data == '?').sum())\n",
    "# print((val_data == '?').sum())\n",
    "# print(train_data.isna().sum())\n",
    "# print(val_data.isna().sum())\n",
    "\n",
    "# set the data to numerics and change ? to na\n",
    "num_train = train_data\n",
    "for col in predictors + [target]:\n",
    "    num_train[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "\n",
    "# replace the ? with mean values\n",
    "for col in predictors:\n",
    "    mean_val = num_train[col].mean()\n",
    "    train_data[col] = train_data[col].replace('?',mean_val)\n",
    "    val_data[col] = val_data[col].replace('?',mean_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Standardize the predictors in the training set using z-score standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the predictors in the training set using z-score standardization.\n",
    "for col in predictors:\n",
    "    print(f'{col}: {np.mean(train_data[col]):.2f} ({np.std(train_data[col]):.2f})')\n",
    "\n",
    "# z-score standardization\n",
    "for col in predictors:\n",
    "    train_data[col] = (train_data[col] - np.mean(train_data[col])) / np.std(train_data[col])\n",
    "\n",
    "for col in predictors:\n",
    "    print(f'{col} after: {np.mean(train_data[col]):.2f} ({np.std(train_data[col]):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Standardize the predictors in validation set based on the means and standard deviations from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cylinders: 0.00 (1.00)\n",
      "displacement: -0.00 (1.00)\n",
      "horsepower: 0.00 (1.00)\n",
      "weight: 0.00 (1.00)\n",
      "acceleration: 0.00 (1.00)\n",
      "year: 0.00 (1.00)\n",
      "cylinders after: 0.00 (1.00)\n",
      "displacement after: -0.00 (1.00)\n",
      "horsepower after: 0.00 (1.00)\n",
      "weight after: 0.00 (1.00)\n",
      "acceleration after: 0.00 (1.00)\n",
      "year after: 0.00 (1.00)\n"
     ]
    }
   ],
   "source": [
    "for col in predictors:\n",
    "    print(f'{col}: {np.mean(train_data[col]):.2f} ({np.std(train_data[col]):.2f})')\n",
    "\n",
    "# z-score standardization\n",
    "for col in predictors:\n",
    "    val_data[col] = (train_data[col] - np.mean(train_data[col])) / np.std(train_data[col])\n",
    "\n",
    "for col in predictors:\n",
    "    print(f'{col} after: {np.mean(train_data[col]):.2f} ({np.std(train_data[col]):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Reflection: Discuss briefly why it is a good idea (or even necessary?) to standardize the variables before fitting the LASSO models in assignment 2. Why do we mean-fill and standardize the validation set based on information from the training set?\n",
    "\n",
    "It is a good idea and necessary to standardize the variables because this gives us the same scale/unit of the coefficients. Shrinkage penalizes  the coefficients directly regardless of the scales. If we do not standardize it, variables on larger scales will be penalized less than those on smaller scales. We use the information from the training set because we want to keep the model agnostic to the validation set during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-for-scicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
