{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Prepare the dataset for the subsequent modelling.**\n",
    "\n",
    "    1. Download the heart disease dataset from https://www.statlearning.com/s/Heart.csv\n",
    "    2. Load the dataset and drop all variables except the predictors Age, Sex, ChestPain, RestBP, Chol, and the target variable AHD. Drop all rows containing a NaN value.\n",
    "    3. Onehot encode the variable ChestPain. This means that where you before had a single column with one of four values ['typical', 'asymptomatic', 'nonanginal', 'nontypical'], you will now have four binary columns (their names don't matter), akin to 'ChestPain_typical' 'ChestPain_asymptomatic', 'ChestPain_nonanginal', 'ChestPain_nontypical'. A row that before had ChestPain='typical' will now have ChestPain_typical=1 and the other three columns set to 0, ChestPain='asymptomatic' will have ChestPain_asymptomatic=1 and the other three set to 0, etc.\n",
    "    4. Binary encode the target variable AHD such that 'No'=0 and 'Yes'=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/vwqvh81108b_mls63cm7cgcw0000gp/T/ipykernel_87181/515935684.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0\n",
      "1      1\n",
      "2      1\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "298    0\n",
      "299    1\n",
      "300    1\n",
      "301    0\n",
      "302    0\n",
      "Name: ChestPain_asymptomatic, Length: 303, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  Heart.iloc[:, -4:] = Heart.iloc[:, -4:].astype(int)\n",
      "/var/folders/sx/vwqvh81108b_mls63cm7cgcw0000gp/T/ipykernel_87181/515935684.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "298    0\n",
      "299    0\n",
      "300    0\n",
      "301    0\n",
      "302    1\n",
      "Name: ChestPain_nonanginal, Length: 303, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  Heart.iloc[:, -4:] = Heart.iloc[:, -4:].astype(int)\n",
      "/var/folders/sx/vwqvh81108b_mls63cm7cgcw0000gp/T/ipykernel_87181/515935684.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "298    0\n",
      "299    0\n",
      "300    0\n",
      "301    1\n",
      "302    0\n",
      "Name: ChestPain_nontypical, Length: 303, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  Heart.iloc[:, -4:] = Heart.iloc[:, -4:].astype(int)\n",
      "/var/folders/sx/vwqvh81108b_mls63cm7cgcw0000gp/T/ipykernel_87181/515935684.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "298    1\n",
      "299    0\n",
      "300    0\n",
      "301    0\n",
      "302    0\n",
      "Name: ChestPain_typical, Length: 303, dtype: int64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  Heart.iloc[:, -4:] = Heart.iloc[:, -4:].astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>AHD</th>\n",
       "      <th>ChestPain_asymptomatic</th>\n",
       "      <th>ChestPain_nonanginal</th>\n",
       "      <th>ChestPain_nontypical</th>\n",
       "      <th>ChestPain_typical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  RestBP  Chol  AHD  ChestPain_asymptomatic  ChestPain_nonanginal  \\\n",
       "0   63    1     145   233    0                       0                     0   \n",
       "1   67    1     160   286    1                       1                     0   \n",
       "2   67    1     120   229    1                       1                     0   \n",
       "3   37    1     130   250    0                       0                     1   \n",
       "4   41    0     130   204    0                       0                     0   \n",
       "\n",
       "   ChestPain_nontypical  ChestPain_typical  \n",
       "0                     0                  1  \n",
       "1                     0                  0  \n",
       "2                     0                  0  \n",
       "3                     0                  0  \n",
       "4                     1                  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1: Load the dataset\n",
    "url = \"https://www.statlearning.com/s/Heart.csv\"\n",
    "Heart = pd.read_csv(url)\n",
    "\n",
    "# 2: Keep relevant columns\n",
    "Heart = Heart[['Age', 'Sex', 'ChestPain', 'RestBP', 'Chol', 'AHD']]\n",
    "\n",
    "# 3: Drop rows with any missing values\n",
    "Heart = Heart.dropna()\n",
    "\n",
    "# 4: One-hot encode 'ChestPain'\n",
    "Heart = pd.get_dummies(Heart, columns=['ChestPain'])\n",
    "Heart.iloc[:, -4:] = Heart.iloc[:, -4:].astype(int)\n",
    "\n",
    "# 5: Binary encode 'AHD': 'No' → 0, 'Yes' → 1\n",
    "Heart['AHD'] = Heart['AHD'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "Heart.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Fit a model using a standard train/validation split through multiple steps.**\n",
    "\n",
    "1.1. Write a function \"stratified_split\" that takes three arguments: A dataframe, a number of folds, and a list of variables to stratify by. The function should return a list of dataframes, one for each fold, where the dataframes are stratified by the variables in the list. Test that the function works by splitting the dataset into two folds based on 'AHD', 'Age' and 'RestBP' and print the size of each fold, the counts of 0s and 1s in AHD, and the mean of each of 'Age' and 'RestBP' (all these should be printed individually per fold). Ensure that the function does not modify the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Size: 152\n",
      "AHD=0 count: 84\n",
      "AHD=1 count: 68\n",
      "Mean Age: 54.66\n",
      "Mean RestBP: 131.25\n",
      "------------------------------\n",
      "Fold 2\n",
      "Size: 151\n",
      "AHD=0 count: 80\n",
      "AHD=1 count: 71\n",
      "Mean Age: 54.22\n",
      "Mean RestBP: 132.13\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiqi/miniforge3/envs/python-for-scicomp/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1: write a function\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "def stratified_split(df, n_folds, stratify_vars):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create a new column that represents the stratification group\n",
    "    df_copy['stratify_group'] = df_copy[stratify_vars].astype(str).agg('-'.join, axis=1)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    folds = []\n",
    "    \n",
    "    for _, test_idx in skf.split(df_copy, df_copy['stratify_group']):\n",
    "        fold_df = df_copy.iloc[test_idx].drop(columns=['stratify_group']).copy()\n",
    "        folds.append(fold_df)\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# 2: test the function \n",
    "# Assume 'Heart' DataFrame is already loaded and preprocessed as per your earlier steps\n",
    "\n",
    "# Apply stratified_split on 'AHD', 'Age', and 'RestBP'\n",
    "folds = stratified_split(Heart, 2, ['AHD', 'Age', 'RestBP'])\n",
    "\n",
    "# Print fold statistics\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    print(f\"Size: {len(fold)}\")\n",
    "    print(f\"AHD=0 count: {(fold['AHD'] == 0).sum()}\")\n",
    "    print(f\"AHD=1 count: {(fold['AHD'] == 1).sum()}\")\n",
    "    print(f\"Mean Age: {fold['Age'].mean():.2f}\")\n",
    "    print(f\"Mean RestBP: {fold['RestBP'].mean():.2f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Write a function 'fit_and_predict' that takes 4 arguments: A training set, a validation set, a list of predictors, and a target variable. The function should fit a logistic regression model to the training set using the predictors and target variable, and return the predictions of the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit_and_predict(train_df, valid_df, predictors, target):\n",
    "    # Create and fit the logistic regression model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(train_df[predictors], train_df[target])\n",
    "    \n",
    "    # Predict probabilities on validation set (only the probability of class 1)\n",
    "    predictions = model.predict_proba(valid_df[predictors])[:, 1]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Write a function 'fit_and_predict_standardized' that takes 5 arguments: A training set, a validation set, a list of predictors, a target variable, and a list of variables to standardize. Using a loop (or a scaler), the function should z-score standardize the given variables in both the training set and the validation set based on the mean and standard deviation in the training set. Then, the function should call the 'fit_and_predict' function and return its result. Ensure that the function does not modify the original dataframes. Test the function using the train and validation set from above (e.g. the two folds from the split), while standardizing the 'Age', 'RestBP' and 'Chol' variables (as mentioned above, the target should be AHD, and you should also include the remaining predictors: 'Sex' and the ChestPain-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8924791  0.87586785 0.29629782 0.57033586 0.85776799]\n"
     ]
    }
   ],
   "source": [
    "# 1: write a function \n",
    "def fit_and_predict_standardized(train_df, valid_df, predictors, target, standardize_vars):\n",
    "    # Copy data to avoid modifying the original dataframes\n",
    "    train_df = train_df.copy()\n",
    "    valid_df = valid_df.copy()\n",
    "    \n",
    "    # Standardize variables using z-score (based on training data)\n",
    "    for var in standardize_vars:\n",
    "        mean = train_df[var].mean()\n",
    "        std = train_df[var].std()\n",
    "        train_df[var] = (train_df[var] - mean) / std\n",
    "        valid_df[var] = (valid_df[var] - mean) / std  # use training stats\n",
    "    \n",
    "    # Fit and predict using logistic regression\n",
    "    return fit_and_predict(train_df, valid_df, predictors, target)\n",
    "\n",
    "# 2: test the function \n",
    "# Define columns\n",
    "standardize_vars = ['Age', 'RestBP', 'Chol']\n",
    "predictors = ['Age', 'Sex', 'RestBP', 'Chol'] + [col for col in Heart.columns if col.startswith('ChestPain_')]\n",
    "target = 'AHD'\n",
    "\n",
    "# Use folds[0] for training, folds[1] for validation\n",
    "predictions = fit_and_predict_standardized(folds[0], folds[1], predictors, target, standardize_vars)\n",
    "\n",
    "# Preview predictions\n",
    "print(predictions[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Write a function 'fit_and_compute_auc' that takes 5 arguments: A training set, a validation set, a list of predictors, a target variable, and a list of variables to standardize. The function should call the 'fit_and_predict_standardized' function to retrieve out-of-sample predictions for the validation set. Based on these and the ground truth labels in the validation set, it should compute and return the AUC. Test the function using the train and test set from above, while standardizing the 'Age', 'RestBP' and 'Chol' variables (and including the remaining predictors). Print the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8528\n"
     ]
    }
   ],
   "source": [
    "# 1: write a function\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def fit_and_compute_auc(train_df, valid_df, predictors, target, standardize_vars):\n",
    "    # Get predicted probabilities\n",
    "    preds = fit_and_predict_standardized(train_df, valid_df, predictors, target, standardize_vars)\n",
    "    \n",
    "    # Compute AUC using ground truth vs predicted probs\n",
    "    auc = roc_auc_score(valid_df[target], preds)\n",
    "    return auc\n",
    "\n",
    "# 2: test the function \n",
    "# Define columns\n",
    "standardize_vars = ['Age', 'RestBP', 'Chol']\n",
    "predictors = ['Age', 'Sex', 'RestBP', 'Chol'] + [col for col in Heart.columns if col.startswith('ChestPain_')]\n",
    "target = 'AHD'\n",
    "\n",
    "# Use two folds created earlier\n",
    "train_set = folds[0]\n",
    "valid_set = folds[1]\n",
    "\n",
    "# Calculate AUC\n",
    "auc = fit_and_compute_auc(train_set, valid_set, predictors, target, standardize_vars)\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Perform a cross-validation.**\n",
    "\n",
    "Use the 'stratified_split' function to split the dataset into 10 folds, stratified on variables you find reasonable. For each fold, use the 'fit_and_compute_auc' function to compute the AUC of the model on the held-out validation set. Print the mean and standard deviation of the AUCs across the 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.7941\n",
      "Fold 2 AUC: 0.8613\n",
      "Fold 3 AUC: 0.7815\n",
      "Fold 4 AUC: 0.8824\n",
      "Fold 5 AUC: 0.8125\n",
      "Fold 6 AUC: 0.8348\n",
      "Fold 7 AUC: 0.9509\n",
      "Fold 8 AUC: 0.8616\n",
      "Fold 9 AUC: 0.9107\n",
      "Fold 10 AUC: 0.7723\n",
      "\n",
      "Mean AUC over 10 folds: 0.8462\n",
      "Standard Deviation of AUC: 0.0552\n"
     ]
    }
   ],
   "source": [
    "cv_folds = stratified_split(Heart, 10, stratify_vars=['AHD'])\n",
    "\n",
    "# Cross-validation loop\n",
    "auc_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Use the i-th fold as validation, the rest as training\n",
    "    valid_fold = cv_folds[i]\n",
    "    train_folds = [cv_folds[j] for j in range(10) if j != i]\n",
    "    train_df = pd.concat(train_folds, ignore_index=True)\n",
    "    \n",
    "    # Compute AUC\n",
    "    auc = fit_and_compute_auc(train_df, valid_fold, predictors, target, standardize_vars)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    print(f\"Fold {i+1} AUC: {auc:.4f}\")\n",
    "\n",
    "# Report mean and standard deviation of AUCs\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "print(f\"\\nMean AUC over 10 folds: {mean_auc:.4f}\")\n",
    "print(f\"Standard Deviation of AUC: {std_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL 3. Use the bootstrap to achieve a distribution of out-of-bag AUCs.\n",
    "For 100 iterations, create a bootstrap sample by sampling with replacement from the full dataset until you have a training set equal in size to 80% of the original data. Use the observations not included in the bootstrap sample as the validation set for that iteration.. Fit models and calculate AUCs for each iteration. Print the mean and standard deviation of the AUCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 AUC: 0.7968\n",
      "Iteration 2 AUC: 0.8515\n",
      "Iteration 3 AUC: 0.7997\n",
      "Iteration 4 AUC: 0.8011\n",
      "Iteration 5 AUC: 0.8064\n",
      "Iteration 6 AUC: 0.7675\n",
      "Iteration 7 AUC: 0.8950\n",
      "Iteration 8 AUC: 0.8694\n",
      "Iteration 9 AUC: 0.8909\n",
      "Iteration 10 AUC: 0.8358\n",
      "Iteration 11 AUC: 0.8166\n",
      "Iteration 12 AUC: 0.8314\n",
      "Iteration 13 AUC: 0.8628\n",
      "Iteration 14 AUC: 0.8316\n",
      "Iteration 15 AUC: 0.8100\n",
      "Iteration 16 AUC: 0.8363\n",
      "Iteration 17 AUC: 0.8283\n",
      "Iteration 18 AUC: 0.8234\n",
      "Iteration 19 AUC: 0.8040\n",
      "Iteration 20 AUC: 0.8292\n",
      "Iteration 21 AUC: 0.8148\n",
      "Iteration 22 AUC: 0.7839\n",
      "Iteration 23 AUC: 0.8609\n",
      "Iteration 24 AUC: 0.8250\n",
      "Iteration 25 AUC: 0.7983\n",
      "Iteration 26 AUC: 0.8336\n",
      "Iteration 27 AUC: 0.8389\n",
      "Iteration 28 AUC: 0.8202\n",
      "Iteration 29 AUC: 0.8054\n",
      "Iteration 30 AUC: 0.8496\n",
      "Iteration 31 AUC: 0.8765\n",
      "Iteration 32 AUC: 0.8249\n",
      "Iteration 33 AUC: 0.8704\n",
      "Iteration 34 AUC: 0.7878\n",
      "Iteration 35 AUC: 0.7990\n",
      "Iteration 36 AUC: 0.8284\n",
      "Iteration 37 AUC: 0.8366\n",
      "Iteration 38 AUC: 0.8316\n",
      "Iteration 39 AUC: 0.8401\n",
      "Iteration 40 AUC: 0.8491\n",
      "Iteration 41 AUC: 0.8239\n",
      "Iteration 42 AUC: 0.8368\n",
      "Iteration 43 AUC: 0.8520\n",
      "Iteration 44 AUC: 0.8076\n",
      "Iteration 45 AUC: 0.8154\n",
      "Iteration 46 AUC: 0.8132\n",
      "Iteration 47 AUC: 0.8171\n",
      "Iteration 48 AUC: 0.8354\n",
      "Iteration 49 AUC: 0.7853\n",
      "Iteration 50 AUC: 0.8487\n",
      "Iteration 51 AUC: 0.8204\n",
      "Iteration 52 AUC: 0.8406\n",
      "Iteration 53 AUC: 0.8383\n",
      "Iteration 54 AUC: 0.8093\n",
      "Iteration 55 AUC: 0.7926\n",
      "Iteration 56 AUC: 0.8227\n",
      "Iteration 57 AUC: 0.8249\n",
      "Iteration 58 AUC: 0.8575\n",
      "Iteration 59 AUC: 0.8327\n",
      "Iteration 60 AUC: 0.8243\n",
      "Iteration 61 AUC: 0.8073\n",
      "Iteration 62 AUC: 0.8381\n",
      "Iteration 63 AUC: 0.8624\n",
      "Iteration 64 AUC: 0.8560\n",
      "Iteration 65 AUC: 0.7848\n",
      "Iteration 66 AUC: 0.7799\n",
      "Iteration 67 AUC: 0.8470\n",
      "Iteration 68 AUC: 0.8046\n",
      "Iteration 69 AUC: 0.8537\n",
      "Iteration 70 AUC: 0.8328\n",
      "Iteration 71 AUC: 0.8488\n",
      "Iteration 72 AUC: 0.8215\n",
      "Iteration 73 AUC: 0.8421\n",
      "Iteration 74 AUC: 0.8641\n",
      "Iteration 75 AUC: 0.8041\n",
      "Iteration 76 AUC: 0.7987\n",
      "Iteration 77 AUC: 0.8386\n",
      "Iteration 78 AUC: 0.8707\n",
      "Iteration 79 AUC: 0.8121\n",
      "Iteration 80 AUC: 0.8173\n",
      "Iteration 81 AUC: 0.8200\n",
      "Iteration 82 AUC: 0.8396\n",
      "Iteration 83 AUC: 0.8149\n",
      "Iteration 84 AUC: 0.7968\n",
      "Iteration 85 AUC: 0.8820\n",
      "Iteration 86 AUC: 0.8520\n",
      "Iteration 87 AUC: 0.8565\n",
      "Iteration 88 AUC: 0.8560\n",
      "Iteration 89 AUC: 0.8316\n",
      "Iteration 90 AUC: 0.8590\n",
      "Iteration 91 AUC: 0.8739\n",
      "Iteration 92 AUC: 0.8549\n",
      "Iteration 93 AUC: 0.8366\n",
      "Iteration 94 AUC: 0.8710\n",
      "Iteration 95 AUC: 0.8453\n",
      "Iteration 96 AUC: 0.8598\n",
      "Iteration 97 AUC: 0.8131\n",
      "Iteration 98 AUC: 0.8082\n",
      "Iteration 99 AUC: 0.8378\n",
      "Iteration 100 AUC: 0.8440\n",
      "\n",
      "Bootstrap Mean AUC: 0.8310\n",
      "Bootstrap Std AUC: 0.0258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_iterations = 100\n",
    "n_size = int(0.8 * len(Heart))\n",
    "\n",
    "# To store AUCs\n",
    "bootstrap_aucs = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Bootstrap sample\n",
    "    train_df = resample(Heart, n_samples=n_size, replace=True, random_state=42 + i)\n",
    "    \n",
    "    # Get out-of-bag samples (not in training set)\n",
    "    oob_indices = list(set(Heart.index) - set(train_df.index))\n",
    "    if not oob_indices:  # Skip if no out-of-bag samples\n",
    "        continue\n",
    "    valid_df = Heart.loc[oob_indices]\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = fit_and_compute_auc(train_df, valid_df, predictors, target, standardize_vars)\n",
    "    bootstrap_aucs.append(auc)\n",
    "\n",
    "    print(f\"Iteration {i+1} AUC: {auc:.4f}\")\n",
    "\n",
    "# Report summary stats\n",
    "mean_bootstrap_auc = np.mean(bootstrap_aucs)\n",
    "std_bootstrap_auc = np.std(bootstrap_aucs)\n",
    "print(f\"\\nBootstrap Mean AUC: {mean_bootstrap_auc:.4f}\")\n",
    "print(f\"Bootstrap Std AUC: {std_bootstrap_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Theory**\n",
    "\n",
    "**4.1. List some benefits of wrapping code in functions rather than copying and pasting it multiple times.**\n",
    "\n",
    "Wrapping codes in functions allow you to reuse code efficiently without having to rewrite it multiple times, reducing the redundancy in the scripts. When there are changes, we only need to change the function once in stead of changing it everywhere. \n",
    "\n",
    "\n",
    "**4.2. Explain three classification metrics and their benefits and drawbacks.**\n",
    "\n",
    "a) Accuracy: quatifies the proportion of correctly classified cases out of all cases. The benefit is that it is very interpretable, while the drawback is that it does not handle the imbalanced classes.\n",
    "b) True positive rate: the proportion of the true positives out of all the ground positives. The benefit is that it is interpretable, and it calculates the proportion of cases that are detected. It is useful when the cost of false negatives is high. However, it only focuses on one side of the classification problem and ignores everything else, like false positives or how well it handles negative cases.\n",
    "c) True negative rate: the proportion of the true negatives out of all ground negatives. It is interpretable, and useful when the cost of false positives is high. Its drawback is that it is also one-sided.\n",
    "\n",
    "\n",
    "**4.3. Write a couple of sentences comparing the three methods (train/validation, cross-validation, bootstrap) above as approaches to quantify model performance. Which one yielded the best results? Which one would you expect to yield the best results? Can you mention some theoretical benefits and drawbacks with each? Even if you didn't do the optional bootstrap exercise you should reflect on this as an approach.**\n",
    "\n",
    "a) the train/validation approach is fast and easy to implement. However it gives onlu one performance estimates, which can vary a lot depending on the split.\n",
    "b) cross-validation uses all the data to train models, providing more robust estimate by averaging over multiple splits. The drawbacks are that a)there are different choices of k yields different results b) multiple models have to be specified.\n",
    "c) bootstrap uses all data to train models, and gives a full distribution of performance metrics, can be used for uncertainty estimation(e.g., confidence intervals). The drawback is that there are different choices of b that yields different results\n",
    "\n",
    "Cross-validation likely yields the most consistent results, while bootstrap provides insight into the variability of model performance. Theoretically, cross-validation is preferrs for model selection, while bootstrap is powerful for uncertainty estimation. The train/validation split, though quick, is the least robust of the three.\n",
    "\n",
    "**4.4. Why do we stratify the dataset before splitting?**\n",
    "\n",
    "Because we want to ensure that all folds of the dataset have similar distributions in some given characteristics, such as the outcome variable, age, sex.\n",
    "\n",
    "**4.5. What other use cases can you think of for the bootstrap method?**\n",
    "\n",
    "We can use the bootstrap to estimate the confidence intervals for statistics such as regression coeficients. Bootstrap can also be used to simulate the null distribution and calculate p-values when traditional tests aren't valid or assumptions are violated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-for-scicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
